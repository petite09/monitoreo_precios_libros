# Monitoreo de precios de libros de mi Wishlist de Buscalibre ‚ú®

Este es un proyecto desarrollado para el Bootcamp de Latinas in Cloud.
La idea consiste en utilizar el lenguaje python para crear el c√≥digo del proyecto y usar los servicios de AWS para su ejecuci√≥n.

El objetivo de este proyecto es recolectar datos diariamente de los precios 4 libros que me interesa comprar y que se env√≠e una notificaci√≥n cuando se detecte una bajada de precio en alguno de los libros.
Adem√°s, quiero poder visualizar semanalmente un gr√°fico que me muestre los precios de los 4 libros en funci√≥n de las fechas en que se recolectaron los datos.
Por el momento, el c√≥digo (**_Monitore_precios_**) solo extrae el precio de los libros de inter√©s desde la p√°gina de buscalibre y guarda los datos (T√≠tulo, precio y fecha) en un archivo CSV (no tengo nada automatizado a√∫n, solo ejecuci√≥n manual).
Tambi√©n tengo un c√≥digo aparte para la visualizaci√≥n de los datos (_**Visualizaci√≥n_precios)**_, lo que me permite generar un gr√°fico para ver la evoluci√≥n de los precios en el tiempo.

A continuaci√≥n dejo los detalles del proyecto, un esquema de c√≥mo ser√≠a la arquitectura en AWS (te√≥ricamente) y lo que queda pendiente del proyecto.

## Requerimientos:

Para este proyecto se utilizaron las siguientes librer√≠as/m√≥dulos/paquetes:

- **requests**: se utiliza para realizar solicitudes HTTP a p√°ginas web. En este caso se utiliz√≥ para enviar solicitudes GET a las URLs de los libros en Buscalibre y obtener el HTML de la p√°gina correspondiente.
- **beautufilsoup4**: sirve para extraer datos de archivos HTML. En este proyecto se us√≥ para buscar y extraer elementos espec√≠ficos (en este caso el elemento <_scripts_> que contiene el objeto "_datalayer_").
- **re**: m√≥dulo que permite buscar patrones en cadena de texto mediante expresiones regulares. En este proyecto se utiliz√≥ para buscar dentro del contenido del <_script_> "datalayer" del HTML el valor del precio con un patr√≥n espec√≠fico.
- **pandas**: se utiliza para manipular y analizar datos. En este caso se emplea para crear y manipular DataFrames, que permiten almacenar los datos de los libros y los precios en un archivo CSV. Tambi√©n se usa para leer y concaternar los datos cuando se agregar nuevas entradas al archivo.
- **datetime**: m√≥dulo que se usa para manejar fechas y horas. En el proyecto se utiliz√≥ para registrar la fecha actual cuando se almacena u nuevo precio en el archivo CSV, permitiendo tener la informaci√≥n para rastrear la evoluci√≥n de los precios en el tiempo.
- **matplotlib**: se usa para crear gr√°ficos y visualizaciones. En este caso se utiliz√≥ para graficar la evoluci√≥n de los precios de los libros a lo largo del tiempo.

## Pasos

### 1. Obtener el HTML de las p√°ginas de los libros en mi lista de deseos

```
def obtener_html(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers`
    
    if response.status_code == 200:
        return response.text
    else:
        print(f"Error al obtener la p√°gina: {response.status_code} para la URL: {url}")
        return None
```

Esta secci√≥n del c√≥digo se utiliza para definir la funci√≥n que se usar√° para hacer una solicitud HTTP a la URL proporcionada y obtener el contenido de la p√°gina.
`"User-Agent"` es una cadena que identifica al cliente que hace la solicitud, simula un navegador web.
`requests.get()` env√≠a una solicitud HTTP GET a la URL proporcionada. 
El argumento headers=headers se pasa para incluir las cabeceras definidas en la solicitud, haciendo que parezca que la solicitud proviene de un navegador.
`response.status_code` devuelve el c√≥digo de estado de la respuesta HTTP. Un c√≥digo 200 significa que la solicitud fue exitosa y la p√°gina se carg√≥ correctamente.
Si el c√≥digo es 200, la funci√≥n devuelve response.text, que es el contenido HTML de la p√°gina como un string.
Si la solicitud no es exitosa (c√≥digo diferente de 200), se imprime un mensaje de error con el c√≥digo de estado y la funci√≥n devuelve `None`.

### 2. Funci√≥n para extraer el precio

```
def extraer_precio(html):
    soup = BeautifulSoup(html, 'html.parser')


    script_tag = soup.find('script', string=re.compile('dataLayer')) #Busca el script que contiene dataLayer

    if script_tag:
        # Usar una expresi√≥n regular para extraer el precio del script
        match = re.search(r"'precio_producto':\s*'(\d+)'", script_tag.string)
        if match:
            precio = match.group(1)  # Captura el valor del precio
            return int(precio)
        else:
            print("No se pudo encontrar el precio en el script.")
            return None
    else:
        print("No se encontr√≥ el script de dataLayer.")
        return None```
``` 
### 3. Funci√≥n para registrar el precio en un archivo CSV

```
def registrar_precio(libro, precio):
    fecha = datetime.now().strftime('%Y-%m-%d')
    data = {'Fecha': [fecha], 'Libro': [libro], 'Precio': [precio]}`

    df = pd.DataFrame(data)

    # Guardar o a√±adir al archivo CSV
    try:
        df_existing = pd.read_csv('precios_libros.csv')
        df = pd.concat([df_existing, df], ignore_index=True)
    except FileNotFoundError:
        pass

    df.to_csv('precios_libros.csv', index=False)
```

## 4. Definir la lista de URLs de los libros de mi lista de deseos
```
libros = [
    {"titulo": "Entremuros", "url": "https://www.buscalibre.cl/libro-entremuros/9789878474458/p/54393883"},
    {"titulo": "El p√°jaro que bebe l√°grimas", "url": "https://www.buscalibre.cl/libro-el-pajaro-que-bebe-lagrimas-n-01-04-el-corazon-del-naga/9788445017098/p/62013231"},
    {"titulo": "La era del mito", "url": "https://www.buscalibre.cl/libro-la-era-del-mito/9789583056949/p/50543764"},
    {"titulo": "El l√≠mite del cielo", "url": "https://www.buscalibre.cl/libro-el-limite-del-cielo/9788410163171/p/56764769"}
]
```
### 5. Iterar sobra cada URL y procesar los datos

```
for libro in libros:
    html = obtener_html(libro["url"])
    if html:
        precio = extraer_precio(html)
        if precio:
            registrar_precio(libro["titulo"], precio)
            print(f"El precio de '{libro['titulo']}' es: {precio}")
        else:
            print(f"No se pudo extraer el precio para el libro: {libro['titulo']}")
    else:
        print(f"No se pudo obtener el HTML para el libro: {libro['titulo']}")
```
### 6. Visualizaci√≥n de los precios

Al inicio ten√≠a este bloque de c√≥digo en el archivo de Monitoreo_precios, que es el script que extrae los precios, pero decid√≠ dejarlo aparte para poder ejecutarlo de manera semanal (con EventBridge) a diferencia del monitoreo de precios que quiero que se haga diariamente.

#Funci√≥n para visualizar la evoluci√≥n de los precios

```
import pandas as pd
import matplotlib.pyplot as plt

def visualizar_evolucion():
    df = pd.read_csv('precios_libros.csv')```

    # Convertir la columna 'Fecha' a tipo datetime
    df['Fecha'] = pd.to_datetime(df['Fecha'])

    # Agrupar por t√≠tulo de libro y graficar cada uno
    for libro in df['Libro'].unique():
        df_libro = df[df['Libro'] == libro]
        plt.plot(df_libro['Fecha'], df_libro['Precio'], marker='o', label=libro)

    # Configuraci√≥n del gr√°fico
    plt.xlabel('Fecha')
    plt.ylabel('Precio')
    plt.title('Evoluci√≥n de precios de los libros')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()

    # Mostrar el gr√°fico
    plt.show()

# Llamada a la funci√≥n para visualizar el gr√°fico
visualizar_evolucion()
```
Ac√° hay un peque√±o ejemplo de visualizaci√≥n:

![img_2.png](img_2.png)

### Arquitectura en AWS (te√≥rico)
Separ√© el proyecto en 2 partes: una de recolecci√≥n, almacenamiento de datos, y notificaci√≥n de bajada de precios y otra parte de visualizaci√≥n de la informaci√≥n.

Ac√° hay un peque√±o diagrama de c√≥mo ser√≠a la arquitectura del proyecto:

![img.png](img.png)

En este primer caso, se configura **EventBridge** para que active la fuci√≥n Lambda (1) que se encarga de recolectar los datos de los precios de manera diaria.
La funci√≥n **Lambda (1)** contiene el c√≥digo para extraer los datos necesarios de las URLs de los libros (como t√≠tulo, precio y fecha de recolecci√≥n) y los almacena en un archivo CSV que se guardar√° en un bucket **S3 (1)**.
Los datos obtenidos tambi√©n se guardar√°n en una tabla en **DynamoDB**. Si se detecta una bajada de precio, se puede configurar para que escriba un atributo adicional en DynamoDB que indique este evento.
**DynamoDB Stream** verifica si hay un evento de bajada de precio y activa una funci√≥n Lambda (2) en caso de detectar una baja de precio.
La siguiente funci√≥n **Lambda (2)**, que se activa por la verificaci√≥n de DynamoDB Stream, contiene el c√≥digo para un SNS que permite notificar esta bajada de precio. 


![img_1.png](img_1.png)

Para este segundo caso, est√° el tema de la visualizaci√≥n de los datos.
La idea es generar gr√°ficos de manera semanal y no diariamente como el caso de la recolecci√≥n de los datos.
Se configura **EventBridge** para que active la funci√≥n Lambda de visualizaci√≥n.
Esta funci√≥n **Lambda (3)** utiliza el archivo CSV almacenado en el bucket **S3 (1)** de la imagen anterior para generar un gr√°fico que muestre los precios de cada libro en funci√≥n de las fechas de recolecci√≥n de datos.
Finalmente, estos gr√°ficos semanales ser√°n almacendados en formato png en otro bucket **S3 (2)**.

### Tareas pendientes

- Me gustar√≠a modificar la parte de la visualizaci√≥n para tener un gr√°fico m√°s bonito.
- Tengo pendiente toda la parte de AWS.
- Debo configurar EventBridge para activar las diferentes funciones.
- Me falta integrar la parte de DynamoDB en el c√≥digo de *Monitoreo_precio*s.
- Tambi√©n tengo que escribir el c√≥digo necesario para hacer el sistema de notificaci√≥n.
- Debo adaptar el c√≥digo que ya tengo para el monitoreo de precios a los servicios de AWS en general.


### √öltimas acotaciones
Me imagino que todo esto podr√≠a hacerse en un solo gran c√≥digo y quiz√°s no depender de tantas funciones Lambda, pero por el momento esta era la forma m√°s f√°cil que ten√≠a mi cerebro para procesar toda esta informaci√≥n.
Seguir√© estudiando para poder agregar lo que me falta y que quede completamente funcional y para poder implementar mejoras al proyecto en general.

Muchas gracias a las chicas que impartieron el bootcamp. Pude confirmar que esto es algo que me gusta y me entretiene mucho y que definitivamente quiero seguir aprendiendo üíñ